# AI-Prompt-Evaluation-and-Code-Review-in-Python
This project simulates the real-world responsibilities of an AI Engineer by reviewing and improving Python code generated by AI models (like ChatGPT or Gemini). The goal is to evaluate the correctness, efficiency, and clarity of code outputs in response to common Python programming prompts, identify potential issues, and rewrite the solutions with best practices in mind.

## Objective
- Evaluate Python code generated by AI prompts
- Identify bugs, inefficiencies, or poor practices
- Improve code readability, performance, and reliability
- Provide clear, documented improvements with test cases and explanations

## Skills Demonstrated
- Python programming fundamentals  
- Code debugging and logical analysis  
- Clean code writing with comments and documentation  
- Input validation and test case coverage  
- Use of Python modules like `re` and `collections`  
- Writing maintainable and readable code  
- Real-world prompt evaluation mindset

## Project Structure
Each file includes:
- Original AI-generated code ("Before")
- Identified issues
- Improved version ("After")
- Test cases and insights

## Tasks Completed

### 1. `palindrome_review.py`
- Cleaned user input using regular expressions
- Correctly handles mixed case, punctuation, and whitespace
- More robust for real-world strings (e.g., `"A man, a plan, a canal: Panama"`)

### 2. `factorial_review.py`
- Fixed off-by-one bug in range loop
- Added input validation for negative or non-integer values
- Ensured correct results for 0 and positive integers

### 3. `char_frequency_review.py`
- Replaced manual counting with `collections.Counter`
- Removed whitespace and normalized letter case
- Produces accurate character counts for typical text inputs

### 4. `sort_numbers_review.py`
- Replaced inefficient nested loops with Pythonâ€™s `sorted()` function
- Added input validation to ensure only integers are passed
- Returns a new list without modifying the original

## Sample Output

### Palindrome
print(is_better_palindrome("A man, a plan, a canal: Panama"))  # True

### Factorial
print(better_factorial(5))  # 120

### Character Frequency
print(better_char_frequency("hello world"))  # {'h':1, 'e':1, 'l':3, ...}

### Sorting
print(better_sort_numbers([5, 2, 9, 1]))  # [1, 2, 5, 9]


## Conclusion
This project demonstrates the ability to evaluate AI-generated Python code, identify issues, and rewrite clean, efficient solutions. It reflects real-world tasks in AI Engineering and Prompt Evaluation roles.


## Future Scope
- Add more Python tasks (e.g., recursion, class-based problems)
- Explore prompt chaining and multi-step AI evaluations










